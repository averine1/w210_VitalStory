{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31adbdb8-9753-42fb-869a-e0e3e3c73c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.175.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f8c7bb-5a53-47b8-8e35-35546f2d7540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::099732224608:role/service-role/AmazonSageMaker-ExecutionRole-20250215T170368\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d094754-4cc3-4f8b-84b6-d1c4376684fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/24/25 00:48:28] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Defaulting to only available Python version: py39                    <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#610\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">610</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/24/25 00:48:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Defaulting to only available Python version: py39                    \u001b]8;id=390671;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=936183;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#610\u001b\\\u001b[2m610\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Defaulting to only supported image scope: gpu.                       <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#534\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">534</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Defaulting to only supported image scope: gpu.                       \u001b]8;id=508188;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=36700;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#534\u001b\\\u001b[2m534\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    " \n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.9.3\"\n",
    ")\n",
    " \n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44dd52-307d-4c74-95e0-318c8de398db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# sagemaker config\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "number_of_gpu = 8\n",
    "health_check_timeout = 1500\n",
    " \n",
    "# Define Model and Endpoint configuration parameter\n",
    "\"\"\"\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"m42-health/med42-70b\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'HUGGING_FACE_HUB_TOKEN': \"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\",\n",
    "  'HF_USE_SAFETENSORS': \"true\"\n",
    "  # ,'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}\n",
    "\"\"\"\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"m42-health/med42-70b\",\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "  'MAX_INPUT_LENGTH': json.dumps(4096),  # Increased input length\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(8192),  # Increased total length\n",
    "  'MAX_NEW_TOKENS': json.dumps(2048),  # Ensure enough generation tokens\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(16384),\n",
    "  'TEMPERATURE': json.dumps(0.7),\n",
    "  'TOP_P': json.dumps(0.9),\n",
    "  'LOG_LEVEL': \"debug\",\n",
    "  #'HUGGING_FACE_HUB_TOKEN': \"hf_***\",\n",
    "  'HF_USE_SAFETENSORS': \"true\"\n",
    "}\n",
    "\n",
    "# check if token is set\n",
    "assert config['HUGGING_FACE_HUB_TOKEN'] != \"<REPLACE WITH YOUR TOKEN>\", \"Please set your Hugging Face Hub token\"\n",
    " \n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f1e3f2-9ba4-428b-8b67-a10137014186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/24/25 00:48:36] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name:                                              <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         huggingface-pytorch-tgi-inference-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-24-00-48-36-806              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/24/25 00:48:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name:                                              \u001b]8;id=486430;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=853817;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         huggingface-pytorch-tgi-inference-\u001b[1;36m2025\u001b[0m-03-24-00-48-36-806              \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/24/25 00:48:37] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#5937\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5937</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         huggingface-pytorch-tgi-inference-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-24-00-48-37-393              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/24/25 00:48:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=607272;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=119098;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#5937\u001b\\\u001b[2m5937\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         huggingface-pytorch-tgi-inference-\u001b[1;36m2025\u001b[0m-03-24-00-48-37-393              \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name                                            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4759\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4759</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         huggingface-pytorch-tgi-inference-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-24-00-48-37-393              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name                                            \u001b]8;id=489048;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=527095;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4759\u001b\\\u001b[2m4759\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         huggingface-pytorch-tgi-inference-\u001b[1;36m2025\u001b[0m-03-24-00-48-37-393              \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------!"
     ]
    }
   ],
   "source": [
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea8a666-d493-41bb-8180-384b0fb00846",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cb0c45-686c-4c79-aa7d-a7d1087affd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/22/25 19:44:27] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">GET</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://api.gradio.app/gradio-messaging/en</span> <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1</span> <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">200 OK\"</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/22/25 19:44:27]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mGET\u001b[0m \u001b[4;38;2;0;105;255mhttps://api.gradio.app/gradio-messaging/en\u001b[0m \u001b[38;2;0;135;0m\"HTTP/1.1\u001b[0m \u001b]8;id=649226;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=913724;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m200 OK\"\u001b[0m                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146/2812265235.py:140: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  demo = gr.ChatInterface(generate, title=\"Chat with Vital Story\", chatbot=gr.Chatbot(layout=\"panel\"))\n",
      "/opt/conda/lib/python3.11/site-packages/gradio/chat_interface.py:317: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/22/25 19:44:29] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">GET</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://api.gradio.app/pkg-version</span> <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 200 OK\"</span> <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/22/25 19:44:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mGET\u001b[0m \u001b[4;38;2;0;105;255mhttps://api.gradio.app/pkg-version\u001b[0m \u001b[38;2;0;135;0m\"HTTP/1.1 200 OK\"\u001b[0m \u001b]8;id=227809;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=801492;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">GET</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">http://127.0.0.1:7860/gradio_api/startup-events</span>      <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mGET\u001b[0m \u001b[4;38;2;0;105;255mhttp://127.0.0.1:7860/gradio_api/startup-events\u001b[0m      \u001b]8;id=304686;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=337650;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">HEAD</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">http://127.0.0.1:7860/</span> <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 200 OK\"</span>            <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mHEAD\u001b[0m \u001b[4;38;2;0;105;255mhttp://127.0.0.1:7860/\u001b[0m \u001b[38;2;0;135;0m\"HTTP/1.1 200 OK\"\u001b[0m            \u001b]8;id=868949;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=966668;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">GET</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://api.gradio.app/v3/tunnel-request</span> <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 </span>  <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">200 OK\"</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mGET\u001b[0m \u001b[4;38;2;0;105;255mhttps://api.gradio.app/v3/tunnel-request\u001b[0m \u001b[38;2;0;135;0m\"HTTP/1.1 \u001b[0m  \u001b]8;id=705850;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=273425;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m200 OK\"\u001b[0m                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">GET</span>                                                      <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64</span>      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mGET\u001b[0m                                                      \u001b]8;id=618128;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=965061;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64\u001b[0m      \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://95411e37445bac10b3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">HEAD</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://95411e37445bac10b3.gradio.live</span> <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 </span>   <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">200 OK\"</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mHEAD\u001b[0m \u001b[4;38;2;0;105;255mhttps://95411e37445bac10b3.gradio.live\u001b[0m \u001b[38;2;0;135;0m\"HTTP/1.1 \u001b[0m   \u001b]8;id=286180;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=382504;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m200 OK\"\u001b[0m                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://95411e37445bac10b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "\n",
    "# hyperparameters for llm\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"stop\": [\"</s>\"],\n",
    "}\n",
    "\n",
    "# system_prompt = \"You are an helpful Medical Assistant, called Vitalstory. Knowing everyting about Medical related.\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "        You are VitalChat, a helpful medical assistant specializing in\n",
    "        healthcare-related questions. Your goal is to collect enough information\n",
    "        from the user about their symptom(s) before providing insights.If a\n",
    "        user's input is vague or lacks details, ask two or three clarifying\n",
    "        questions before proceeding.Once you have enough context, generate five\n",
    "        follow-up questions to gather more information.After the three followup\n",
    "        questions create a summary and advice on next steps\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Helper for reading lines from a stream\n",
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                print(\"Unknown event type:\" + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n",
    "\n",
    "\n",
    "# define format function for our input\n",
    "def format_prompt(user_input, history, system_prompt):\n",
    "    \"\"\"\n",
    "    Formats the conversation history and user input using a structured instruction format.\n",
    "    This approach improves the model's ability to follow instructions and ask clarifying questions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the prompt with system instructions\n",
    "    prompt = f\"<|system|>\\n{system_prompt}\\n<|system|>\\n\\n\"\n",
    "\n",
    "    # Ensure history is properly formatted as [(user_input, bot_response), ...]\n",
    "    if not isinstance(history, list):\n",
    "        history = []\n",
    "    formatted_history = []\n",
    "    for entry in history:\n",
    "        #print(entry)\n",
    "        #print(len(entry))\n",
    "        #if isinstance(entry, dict) and len(entry) == 4 and all(isinstance(x, str) for x in entry):\n",
    "        if len(entry) == 2 and all(isinstance(x, str) for x in entry):\n",
    "            formatted_history.append(entry)  # Valid tuple\n",
    "        else:\n",
    "            print(f\"⚠️ Invalid history entry: {entry}, resetting history.\")\n",
    "            history = []  # Reset history if invalid\n",
    "            break  # Prevent partial corruption\n",
    "\n",
    "    # Append formatted history using structured instruction format\n",
    "    for user_text, bot_response in formatted_history:\n",
    "        prompt += f\"<|prompter|>\\n{prompt}\\n<|prompter|>\\n\"\n",
    "        prompt += f\"<|prompter|>\\n{user_text}\\n<|prompter|>\\n\"\n",
    "        prompt += f\"<|assistant|>\\n{bot_response}\\n<|assistant|>\\n\"\n",
    "\n",
    "    # Append the new user input with instruction\n",
    "    prompt += f\"<|prompter|>\\n{user_input}\\n<|prompter|>\\n\"\n",
    "    prompt += \"<|assistant|>\\n\\n<|assistant|>\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def create_gradio_app(\n",
    "    endpoint_name,\n",
    "    session=boto3,\n",
    "    parameters=parameters,\n",
    "    system_prompt=system_prompt,\n",
    "    format_prompt=format_prompt,\n",
    "    concurrency_count=4,\n",
    "    share=True,\n",
    "):\n",
    "    smr = session.client(\"sagemaker-runtime\")\n",
    "\n",
    "    def generate(\n",
    "        prompt,\n",
    "        history,\n",
    "    ):\n",
    "        formatted_prompt = format_prompt(prompt, history, system_prompt)\n",
    "\n",
    "        request = {\"inputs\": formatted_prompt, \"parameters\": parameters, \"stream\": True}\n",
    "        resp = smr.invoke_endpoint_with_response_stream(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(request),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "        output = \"\"\n",
    "        for c in LineIterator(resp[\"Body\"]):\n",
    "            c = c.decode(\"utf-8\")\n",
    "            if c.startswith(\"data:\"):\n",
    "                chunk = json.loads(c.lstrip(\"data:\").rstrip(\"/n\"))\n",
    "                if chunk[\"token\"][\"special\"]:\n",
    "                    continue\n",
    "                if chunk[\"token\"][\"text\"] in request[\"parameters\"][\"stop\"]:\n",
    "                    break\n",
    "                output += chunk[\"token\"][\"text\"]\n",
    "                for stop_str in request[\"parameters\"][\"stop\"]:\n",
    "                    if output.endswith(stop_str):\n",
    "                        output = output[: -len(stop_str)]\n",
    "                        output = output.rstrip()\n",
    "                        yield output\n",
    "\n",
    "                yield output\n",
    "        return output\n",
    "\n",
    "    demo = gr.ChatInterface(generate, title=\"Chat with Vital Story\", chatbot=gr.Chatbot(layout=\"panel\"))\n",
    "    demo.queue().launch(share=share)\n",
    "    #demo.queue(concurrency_count=concurrency_count).launch(share=share)\n",
    "\n",
    "# create gradio app\n",
    "create_gradio_app(\n",
    "    llm.endpoint_name,\n",
    "    session=sess.boto_session,\n",
    "    parameters=parameters,\n",
    "    system_prompt=None,\n",
    "    format_prompt=format_prompt,\n",
    "    concurrency_count=4,\n",
    "    share=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41deda7a-1a64-42c4-a76c-6db76f0c3c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
