{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ba314a-3bfd-4531-bfde-90f24cc2abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker -U --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f58da1-9798-4b74-9c1d-32a0af541030",
   "metadata": {},
   "source": [
    "#### Before we begin with the actual work for packaging and deploying the model to Amazon SageMaker, we need to setup the notebook environment. \n",
    "\n",
    "#### This includes:\n",
    "#### 1) Execution role for SageMaker Studio\n",
    "#### 2) Bucket \n",
    "#### 3) chosen region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1218970-fd86-4f8e-b3e2-20cc8ca12e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::099732224608:role/service-role/AmazonSageMaker-ExecutionRole-20250215T170368\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62732f07-75a2-4379-8b3d-d1bc2fa7a01e",
   "metadata": {},
   "source": [
    "### Get the container URI and leverage this to HF model to point to that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f818f2-3054-4dec-ada4-84de67429aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/06/25 22:53:52] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Defaulting to only available Python version: py39                    <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#610\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">610</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/06/25 22:53:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Defaulting to only available Python version: py39                    \u001b]8;id=970873;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=621350;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#610\u001b\\\u001b[2m610\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Defaulting to only supported image scope: gpu.                       <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#534\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">534</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Defaulting to only supported image scope: gpu.                       \u001b]8;id=711748;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=233979;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#534\u001b\\\u001b[2m534\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.1.0\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ccf8f-9a9e-46b1-8e1d-4309b3ca1cd7",
   "metadata": {},
   "source": [
    "### Step 2: Deploying the Model from Hugging Face Hub\n",
    "### Amazon SageMaker allows direct deployment of models from the Hugging Face Model Hub. For large models like Med42-70B, it's essential to use the Large Model Inference (LMI) container provided by SageMaker, which is optimized for such deployments.\n",
    "\n",
    "### Define the Model and Deployment Configuration: Utilize the HuggingFaceModel class from the SageMaker SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c49f8-b70f-49cf-84af-cc80137c4d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "import json\n",
    "import os\n",
    "# Set CUDA memory allocation configuration to avoid fragmentation\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "# Define the IAM role with necessary permissions\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Hub model configuration\n",
    "hub = {\n",
    "   #'HF_MODEL_ID': 'm42-health/med42-70b',  # Model ID from Hugging Face\n",
    "   #'HF_MODEL_ID': 'm42-health/Llama3-Med42-70B',\n",
    "   'SM_NUM_GPUS': json.dumps(1),\n",
    "   'HF_MODEL_ID': 'm42-health/Llama3-Med42-8B',\n",
    "   'HF_TASK':'question-answering', # Task for the model\n",
    "   #'HF_API_TOKEN': '' , # Replace with your actual token\n",
    "   #'HF_MODEL_QUANTIZE': \"eetq\"\n",
    "   #'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize \n",
    "   #'QUANTIZE' : \"4bit\",\n",
    "   #'BITSANDBYTES_USE_4BIT': \"true\",\n",
    "   #'DEVICE_MAP': \"auto\"\n",
    "}\n",
    "\n",
    "# Specify the Hugging Face TGI inference container URI\n",
    "image_uri = llm_image\n",
    "\n",
    "# Create Hugging Face Model Class with the specified container\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    env=hub,  # Configuration for loading model from Hub\n",
    "    role=role,  # IAM role with permissions\n",
    "    image_uri=image_uri,  # Use the TGI inference container\n",
    "    pytorch_version=\"2.0.0\",  # PyTorch version\n",
    "    py_version='py310',  # Python version        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac29d72-bcf0-448d-ae44-4cb871f99808",
   "metadata": {},
   "source": [
    "### Deploy the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b2ceb3b-7f8b-4df4-bdc8-e277908dd2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/06/25 22:53:54] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name:                                              <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         huggingface-pytorch-tgi-inference-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-04-06-22-53-54-766              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/06/25 22:53:54]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name:                                              \u001b]8;id=975331;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=526947;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         huggingface-pytorch-tgi-inference-\u001b[1;36m2025\u001b[0m-04-06-22-53-54-766              \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/06/25 22:53:55] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#6019\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6019</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         vitalstorymed4270B2025-04-06-22-53-54                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/06/25 22:53:55]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=414182;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=656834;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#6019\u001b\\\u001b[2m6019\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         vitalstorymed4270B2025-04-06-22-53-54                                  \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name vitalstorymed4270B2025-04-06-22-53-54      <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4841\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4841</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name vitalstorymed4270B2025-04-06-22-53-54      \u001b]8;id=465498;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=435152;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4841\u001b\\\u001b[2m4841\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# Deploy the model to a SageMaker endpoint\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "endpoint_name = \"vitalstorymed4270B\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    #instance_type=\"ml.p4d.24xlarge\",  # Instance type with sufficient GPU resources\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "    endpoint_name=endpoint_name  # Name of the endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f02c76fe-c006-45a3-9f9b-49bcc53a14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a medical question-generation assistant.\n",
    "\n",
    "Given a patient health log, generate 3 medically relevant follow-up questions. Be concise and only ask short questions. Do **not** give any advice. Do **not** repeat the patient log or the instructions in your response. Only return valid JSON.\n",
    "\n",
    "The output must follow this format:\n",
    "```json\n",
    "{{\n",
    "  \"questions\": [\n",
    "    \"First follow-up question here?\",\n",
    "    \"Second follow-up question here?\",\n",
    "    \"Third follow-up question here?\"\n",
    "  ]\n",
    "}}```\n",
    "---\n",
    "Example 1:\n",
    "Patient Health Log: \"My stomach hurts after I eat anything, and I feel bloated all the time.\"\n",
    "Response:\n",
    "```json\n",
    "{{\n",
    "  \"questions\": [\n",
    "    \"What types of foods trigger your symptoms?\",\n",
    "    \"Do you experience nausea or vomiting?\",\n",
    "    \"Have you had any recent changes in bowel habits?\"\n",
    "  ]\n",
    "}}```\n",
    "---\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Patient Health Log: \"I keep getting migraines that last all day and don’t respond to painkillers.\"\n",
    "\n",
    "Response:\n",
    "```json\n",
    "{{\n",
    "  \"questions\": [\n",
    "    \"How frequently do the migraines occur?\",\n",
    "    \"Do you notice any warning signs before they start?\",\n",
    "    \"Have you tried any treatments other than painkillers?\"\n",
    "  ]\n",
    "}}```\n",
    "\n",
    "---\n",
    "\n",
    "Now, generate follow-up questions for the following:\n",
    "\n",
    "Patient Health Log: {health_log}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt + \" I have a fever\",\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.6,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_k\": 50,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cff4074-7bc3-47af-99ff-e56c0a5a634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '0485fcb9-6507-4906-a9f4-9bd8656d7307', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0485fcb9-6507-4906-a9f4-9bd8656d7307', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Sun, 06 Apr 2025 02:57:09 GMT', 'content-type': 'application/json', 'content-length': '1415', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f3fea71e1d0>}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "ENDPOINT = 'vitalstorymed4270B2025-04-06-02-45-59'\n",
    "runtime = boto3.client('runtime.sagemaker')\n",
    "response = runtime.invoke_endpoint(EndpointName = ENDPOINT, ContentType = \"application/json\",Body = json.dumps(payload))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c731fbdd-a2ab-4257-bf05-b6fe3fe20540",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = json.loads(response['Body'].read().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f371d95-2052-4e07-aa92-46228afcf5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a medical question-generation assistant.\\n\\nGiven a patient health log, generate 3 medically relevant follow-up questions. Be concise and only ask short questions. Do **not** give any advice. Do **not** repeat the patient log or the instructions in your response. Only return valid JSON.\\n\\nThe output must follow this format:\\n```json\\n{{\\n  \"questions\": [\\n    \"First follow-up question here?\",\\n    \"Second follow-up question here?\",\\n    \"Third follow-up question here?\"\\n  ]\\n}}```\\n---\\nExample 1:\\nPatient Health Log: \"My stomach hurts after I eat anything, and I feel bloated all the time.\"\\nResponse:\\n```json\\n{{\\n  \"questions\": [\\n    \"What types of foods trigger your symptoms?\",\\n    \"Do you experience nausea or vomiting?\",\\n    \"Have you had any recent changes in bowel habits?\"\\n  ]\\n}}```\\n---\\n\\nExample 2:\\n\\nPatient Health Log: \"I keep getting migraines that last all day and don’t respond to painkillers.\"\\n\\nResponse:\\n```json\\n{{\\n  \"questions\": [\\n    \"How frequently do the migraines occur?\",\\n    \"Do you notice any warning signs before they start?\",\\n    \"Have you tried any treatments other than painkillers?\"\\n  ]\\n}}```\\n\\n---\\n\\nNow, generate follow-up questions for the following:\\n\\nPatient Health Log: {health_log}\\n\\nResponse:\\n I have a fever, headache, and sore throat. I also have a cough that produces yellow mucus.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44e14563-6d67-45ee-8c6c-a035f4bbb6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/06/25 03:00:33] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">GET</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://api.gradio.app/gradio-messaging/en</span> <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1</span> <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">200 OK\"</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/06/25 03:00:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mGET\u001b[0m \u001b[4;38;2;0;105;255mhttps://api.gradio.app/gradio-messaging/en\u001b[0m \u001b[38;2;0;135;0m\"HTTP/1.1\u001b[0m \u001b]8;id=335203;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=343051;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m200 OK\"\u001b[0m                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_189/3172541381.py:139: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  demo = gr.ChatInterface(generate, title=\"Chat with Vital Story\", chatbot=gr.Chatbot(layout=\"panel\"))\n",
      "/opt/conda/lib/python3.11/site-packages/gradio/chat_interface.py:317: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/06/25 03:00:35] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">GET</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://api.gradio.app/pkg-version</span> <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 200 OK\"</span> <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/06/25 03:00:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mGET\u001b[0m \u001b[4;38;2;0;105;255mhttps://api.gradio.app/pkg-version\u001b[0m \u001b[38;2;0;135;0m\"HTTP/1.1 200 OK\"\u001b[0m \u001b]8;id=595174;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=600435;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">GET</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">http://127.0.0.1:7860/gradio_api/startup-events</span>      <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mGET\u001b[0m \u001b[4;38;2;0;105;255mhttp://127.0.0.1:7860/gradio_api/startup-events\u001b[0m      \u001b]8;id=547511;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=218548;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">HEAD</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">http://127.0.0.1:7860/</span> <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 200 OK\"</span>            <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mHEAD\u001b[0m \u001b[4;38;2;0;105;255mhttp://127.0.0.1:7860/\u001b[0m \u001b[38;2;0;135;0m\"HTTP/1.1 200 OK\"\u001b[0m            \u001b]8;id=285543;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=367109;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">GET</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://api.gradio.app/v3/tunnel-request</span> <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 </span>  <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">200 OK\"</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mGET\u001b[0m \u001b[4;38;2;0;105;255mhttps://api.gradio.app/v3/tunnel-request\u001b[0m \u001b[38;2;0;135;0m\"HTTP/1.1 \u001b[0m  \u001b]8;id=481427;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=13074;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m200 OK\"\u001b[0m                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">GET</span>                                                      <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64</span>      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mGET\u001b[0m                                                      \u001b]8;id=754402;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=469498;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64\u001b[0m      \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://63b80fe788bc655425.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> HTTP Request: <span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">HEAD</span> <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://63b80fe788bc655425.gradio.live</span> <span style=\"color: #008700; text-decoration-color: #008700\">\"HTTP/1.1 </span>   <a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">200 OK\"</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m HTTP Request: \u001b[1;38;2;215;175;0mHEAD\u001b[0m \u001b[4;38;2;0;105;255mhttps://63b80fe788bc655425.gradio.live\u001b[0m \u001b[38;2;0;135;0m\"HTTP/1.1 \u001b[0m   \u001b]8;id=551541;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=64100;file:///opt/conda/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m200 OK\"\u001b[0m                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://63b80fe788bc655425.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "\n",
    "# hyperparameters for llm\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"stop\": [\"</s>\"],\n",
    "}\n",
    "\n",
    "# system_prompt = \"You are an helpful Medical Assistant, called Vitalstory. Knowing everyting about Medical related.\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "        You are VitalChat, a helpful medical assistant specializing in\n",
    "        healthcare-related questions. Your goal is to collect enough information\n",
    "        from the user about their symptom(s) before providing insights.If a\n",
    "        user's input is vague or lacks details, ask two or three clarifying\n",
    "        questions before proceeding.Once you have enough context, generate five\n",
    "        follow-up questions to gather more information.After the three followup\n",
    "        questions create a summary and advice on next steps\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Helper for reading lines from a stream\n",
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                print(\"Unknown event type:\" + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n",
    "\n",
    "\n",
    "# define format function for our input\n",
    "def format_prompt(user_input, history, system_prompt):\n",
    "    \"\"\"\n",
    "    Formats the conversation history and user input using a structured instruction format.\n",
    "    This approach improves the model's ability to follow instructions and ask clarifying questions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the prompt with system instructions\n",
    "    prompt = f\"<|system|>\\n{system_prompt}\\n<|system|>\\n\\n\"\n",
    "\n",
    "    # Ensure history is properly formatted as [(user_input, bot_response), ...]\n",
    "    if not isinstance(history, list):\n",
    "        history = []\n",
    "    formatted_history = []\n",
    "    for entry in history:\n",
    "        #print(entry)\n",
    "        #print(len(entry))\n",
    "        #if isinstance(entry, dict) and len(entry) == 4 and all(isinstance(x, str) for x in entry):\n",
    "        if len(entry) == 2 and all(isinstance(x, str) for x in entry):\n",
    "            formatted_history.append(entry)  # Valid tuple\n",
    "        else:\n",
    "            print(f\"⚠️ Invalid history entry: {entry}, resetting history.\")\n",
    "            history = []  # Reset history if invalid\n",
    "            break  # Prevent partial corruption\n",
    "\n",
    "    # Append formatted history using structured instruction format\n",
    "    for user_text, bot_response in formatted_history:\n",
    "        prompt += f\"<|prompter|>\\n{user_text}\\n<|prompter|>\\n\"\n",
    "        prompt += f\"<|assistant|>\\n{bot_response}\\n<|assistant|>\\n\"\n",
    "\n",
    "    # Append the new user input with instruction\n",
    "    prompt += f\"<|prompter|>\\n{user_input}\\n<|prompter|>\\n\"\n",
    "    prompt += \"<|assistant|>\\n\\n<|assistant|>\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def create_gradio_app(\n",
    "    endpoint_name,\n",
    "    session=boto3,\n",
    "    parameters=parameters,\n",
    "    system_prompt=system_prompt,\n",
    "    format_prompt=format_prompt,\n",
    "    concurrency_count=4,\n",
    "    share=True,\n",
    "):\n",
    "    smr = session.client(\"sagemaker-runtime\")\n",
    "\n",
    "    def generate(\n",
    "        prompt,\n",
    "        history,\n",
    "    ):\n",
    "        formatted_prompt = format_prompt(prompt, history, system_prompt)\n",
    "\n",
    "        request = {\"inputs\": formatted_prompt, \"parameters\": parameters, \"stream\": True}\n",
    "        resp = smr.invoke_endpoint_with_response_stream(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(request),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "        output = \"\"\n",
    "        for c in LineIterator(resp[\"Body\"]):\n",
    "            c = c.decode(\"utf-8\")\n",
    "            if c.startswith(\"data:\"):\n",
    "                chunk = json.loads(c.lstrip(\"data:\").rstrip(\"/n\"))\n",
    "                if chunk[\"token\"][\"special\"]:\n",
    "                    continue\n",
    "                if chunk[\"token\"][\"text\"] in request[\"parameters\"][\"stop\"]:\n",
    "                    break\n",
    "                output += chunk[\"token\"][\"text\"]\n",
    "                for stop_str in request[\"parameters\"][\"stop\"]:\n",
    "                    if output.endswith(stop_str):\n",
    "                        output = output[: -len(stop_str)]\n",
    "                        output = output.rstrip()\n",
    "                        yield output\n",
    "\n",
    "                yield output\n",
    "        return output\n",
    "\n",
    "    demo = gr.ChatInterface(generate, title=\"Chat with Vital Story\", chatbot=gr.Chatbot(layout=\"panel\"))\n",
    "    demo.queue().launch(share=share)\n",
    "    #demo.queue(concurrency_count=concurrency_count).launch(share=share)\n",
    "\n",
    "# create gradio app\n",
    "create_gradio_app(\n",
    "    predictor.endpoint_name,\n",
    "    session=sess.boto_session,\n",
    "    parameters=parameters,\n",
    "    system_prompt=None,\n",
    "    format_prompt=format_prompt,\n",
    "    concurrency_count=4,\n",
    "    share=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab03bc-8d29-490f-8c9c-1d38fcb1e4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d9047-4671-4f8f-97dc-164ea3648545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
